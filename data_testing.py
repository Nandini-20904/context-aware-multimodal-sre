"""
DATASET TESTING PAGE - UPLOAD AND ANALYZE DATA
Production-ready page for uploading datasets and getting predictions
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import io
import json
import time

def dataset_testing_page():
    """Dataset testing page with file upload and analysis"""
    st.markdown('<h1 class="main-header">üìÅ Dataset Testing & Analysis</h1>', unsafe_allow_html=True)
    
    # Check if system is initialized
    if not st.session_state.get('system_initialized', False):
        st.error("‚ùå System not initialized. Please go to Dashboard first.")
        return
    
    # File upload section
    st.markdown("### üì§ Upload Your Dataset")
    st.markdown("Upload your dataset to analyze for anomalies, failures, and zero-day threats.")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # File uploader
        uploaded_file = st.file_uploader(
            "Choose a file",
            type=['csv', 'xlsx', 'json', 'parquet'],
            help="Upload CSV, Excel, JSON, or Parquet files for analysis"
        )
        
        # Sample data option
        use_sample_data = st.checkbox("Use sample SRE data for testing")
        
        if use_sample_data:
            st.info("‚ÑπÔ∏è Using sample SRE data for demonstration")
    
    with col2:
        st.markdown("#### üìä Supported Formats")
        st.markdown("""
        - **CSV**: Comma-separated values
        - **Excel**: .xlsx files
        - **JSON**: JSON format
        - **Parquet**: Columnar format
        
        #### üìã Expected Columns
        - `cpu_util`: CPU utilization (%)
        - `memory_util`: Memory usage (%)
        - `error_rate`: Error rate (0-1)
        - `timestamp`: Time column (optional)
        """)
    
    # Process uploaded file or sample data
    df = None
    data_source = ""
    
    if uploaded_file is not None:
        try:
            # Read the uploaded file
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
                data_source = f"Uploaded CSV: {uploaded_file.name}"
            elif uploaded_file.name.endswith('.xlsx'):
                df = pd.read_excel(uploaded_file)
                data_source = f"Uploaded Excel: {uploaded_file.name}"
            elif uploaded_file.name.endswith('.json'):
                df = pd.read_json(uploaded_file)
                data_source = f"Uploaded JSON: {uploaded_file.name}"
            elif uploaded_file.name.endswith('.parquet'):
                df = pd.read_parquet(uploaded_file)
                data_source = f"Uploaded Parquet: {uploaded_file.name}"
            
            st.success(f"‚úÖ Successfully loaded {len(df)} rows from {uploaded_file.name}")
            
        except Exception as e:
            st.error(f"‚ùå Error reading file: {e}")
            return
    
    elif use_sample_data:
        # Generate sample SRE data
        np.random.seed(42)
        n_samples = 1000
        
        df = pd.DataFrame({
            'timestamp': pd.date_range(start=datetime.now(), periods=n_samples, freq='1min'),
            'cpu_util': np.random.uniform(20, 100, n_samples),
            'memory_util': np.random.uniform(30, 95, n_samples),
            'error_rate': np.random.exponential(0.02, n_samples),
            'disk_io': np.random.uniform(10, 90, n_samples),
            'network_in': np.random.uniform(100, 1000, n_samples),
            'network_out': np.random.uniform(50, 800, n_samples),
            'response_time': np.random.exponential(0.2, n_samples),
            'active_connections': np.random.poisson(50, n_samples)
        })
        
        # Add some anomalies
        anomaly_indices = np.random.choice(n_samples, size=50, replace=False)
        df.loc[anomaly_indices, 'cpu_util'] = np.random.uniform(95, 100, 50)
        df.loc[anomaly_indices, 'error_rate'] = np.random.uniform(0.1, 0.5, 50)
        
        data_source = "Sample SRE Data (1000 records)"
        st.success(f"‚úÖ Generated sample dataset with {len(df)} records")
    
    # If we have data, show analysis options
    if df is not None:
        st.markdown("---")
        
        # Data preview
        with st.expander("üëÄ Data Preview", expanded=True):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Total Records", len(df))
            with col2:
                st.metric("Columns", len(df.columns))
            with col3:
                st.metric("Data Source", data_source.split(':')[0])
            
            st.markdown("**First 5 rows:**")
            st.dataframe(df.head(), use_container_width=True)
            
            st.markdown("**Data Info:**")
            buffer = io.StringIO()
            df.info(buf=buffer)
            info_str = buffer.getvalue()
            st.text(info_str)
        
        # Data quality check
        st.markdown("### üîç Data Quality Check")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            missing_values = df.isnull().sum().sum()
            st.metric("Missing Values", missing_values, 
                     delta="Good" if missing_values == 0 else "Needs Attention",
                     delta_color="normal" if missing_values == 0 else "inverse")
        
        with col2:
            duplicate_rows = df.duplicated().sum()
            st.metric("Duplicate Rows", duplicate_rows,
                     delta="Good" if duplicate_rows == 0 else "Needs Cleaning",
                     delta_color="normal" if duplicate_rows == 0 else "inverse")
        
        with col3:
            numeric_cols = len(df.select_dtypes(include=[np.number]).columns)
            st.metric("Numeric Columns", numeric_cols)
        
        with col4:
            data_health = "‚úÖ Excellent" if missing_values == 0 and duplicate_rows == 0 else "‚ö†Ô∏è Fair" if missing_values < 10 else "‚ùå Poor"
            st.metric("Data Health", data_health)
        
        # Analysis configuration
        st.markdown("---")
        st.markdown("### ‚öôÔ∏è Analysis Configuration")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("#### üìä Anomaly Detection")
            anomaly_sensitivity = st.slider("Anomaly Sensitivity", 0.1, 1.0, 0.5, 0.1,
                                           help="Higher values detect more anomalies")
            enable_anomaly = st.checkbox("Enable Anomaly Detection", value=True)
        
        with col2:
            st.markdown("#### ‚ö†Ô∏è Failure Prediction")
            failure_threshold = st.slider("Failure Threshold", 0.1, 1.0, 0.5, 0.1,
                                        help="Threshold for failure prediction")
            enable_failure = st.checkbox("Enable Failure Prediction", value=True)
        
        with col3:
            st.markdown("#### üõ°Ô∏è Zero-Day Detection")
            zeroday_sensitivity = st.slider("Zero-Day Sensitivity", 0.1, 1.0, 0.3, 0.1,
                                          help="Higher values detect more potential threats")
            enable_zeroday = st.checkbox("Enable Zero-Day Detection", value=True)
        
        # Run analysis button
        st.markdown("---")
        
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            if st.button("üöÄ Run Complete Analysis", type="primary", use_container_width=True):
                run_complete_analysis(df, enable_anomaly, enable_failure, enable_zeroday,
                                    anomaly_sensitivity, failure_threshold, zeroday_sensitivity)
        
        # Training section
        st.markdown("---")
        st.markdown("### üß† Model Training Options")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üéØ Train on This Data")
            st.markdown("""
            Train the self-learning models using this dataset to improve their accuracy
            and learn new patterns specific to your environment.
            """)
            
            if st.button("üß† Train Models on Dataset", use_container_width=True):
                train_models_on_data(df)
        
        with col2:
            st.markdown("#### üìä Pattern Extraction")
            st.markdown("""
            Extract and analyze patterns from this dataset without full retraining.
            Useful for quick insights and pattern discovery.
            """)
            
            if st.button("üîç Extract Patterns", use_container_width=True):
                extract_patterns_from_data(df)

def run_complete_analysis(df, enable_anomaly, enable_failure, enable_zeroday, 
                         anomaly_sensitivity, failure_threshold, zeroday_sensitivity):
    """Run complete analysis on the uploaded dataset"""
    
    with st.spinner("üîÑ Running comprehensive analysis..."):
        try:
            system = st.session_state.system
            
            # Prepare data for analysis
            analysis_data = prepare_data_for_analysis(df)
            
            # Progress bar
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            results = {}
            
            # Step 1: Anomaly Detection
            if enable_anomaly:
                status_text.text("üîç Running anomaly detection...")
                progress_bar.progress(25)
                
                try:
                    anomaly_results = run_anomaly_detection(analysis_data, anomaly_sensitivity)
                    results['anomaly'] = anomaly_results
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Anomaly detection failed: {e}")
                    results['anomaly'] = None
            
            # Step 2: Failure Prediction
            if enable_failure:
                status_text.text("‚ö†Ô∏è Running failure prediction...")
                progress_bar.progress(50)
                
                try:
                    failure_results = run_failure_prediction(analysis_data, failure_threshold)
                    results['failure'] = failure_results
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Failure prediction failed: {e}")
                    results['failure'] = None
            
            # Step 3: Zero-Day Detection
            if enable_zeroday:
                status_text.text("üõ°Ô∏è Running zero-day detection...")
                progress_bar.progress(75)
                
                try:
                    zeroday_results = run_zeroday_detection(analysis_data, zeroday_sensitivity)
                    results['zeroday'] = zeroday_results
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Zero-day detection failed: {e}")
                    results['zeroday'] = None
            
            # Step 4: Generate Report
            status_text.text("üìä Generating analysis report...")
            progress_bar.progress(100)
            
            time.sleep(1)
            progress_bar.empty()
            status_text.empty()
            
            # Display results
            display_analysis_results(df, results)
            
            st.success("‚úÖ Analysis completed successfully!")
            
        except Exception as e:
            st.error(f"‚ùå Analysis failed: {e}")

def prepare_data_for_analysis(df):
    """Prepare data for analysis by the self-learning system"""
    
    # Ensure required columns exist
    required_cols = ['cpu_util', 'memory_util', 'error_rate']
    
    for col in required_cols:
        if col not in df.columns:
            st.warning(f"‚ö†Ô∏è Missing column '{col}', generating synthetic data")
            if col == 'cpu_util':
                df[col] = np.random.uniform(30, 80, len(df))
            elif col == 'memory_util':
                df[col] = np.random.uniform(40, 75, len(df))
            elif col == 'error_rate':
                df[col] = np.random.exponential(0.02, len(df))
    
    # Clean data
    df = df.fillna(0)
    df = df.replace([np.inf, -np.inf], 0)
    
    # Clip values to reasonable ranges
    if 'cpu_util' in df.columns:
        df['cpu_util'] = np.clip(df['cpu_util'], 0, 100)
    if 'memory_util' in df.columns:
        df['memory_util'] = np.clip(df['memory_util'], 0, 100)
    if 'error_rate' in df.columns:
        df['error_rate'] = np.clip(df['error_rate'], 0, 1)
    
    return df

def run_anomaly_detection(df, sensitivity):
    """Run anomaly detection on the dataset"""
    
    system = st.session_state.system
    
    if system.anomaly_detector and hasattr(system.anomaly_detector, 'is_trained'):
        if system.anomaly_detector.is_trained:
            # Use trained model
            results = system.anomaly_detector.detect_anomalies(df)
        else:
            # Simulate anomaly detection
            results = simulate_anomaly_detection(df, sensitivity)
    else:
        # Simulate anomaly detection
        results = simulate_anomaly_detection(df, sensitivity)
    
    return results

def run_failure_prediction(df, threshold):
    """Run failure prediction on the dataset"""
    
    system = st.session_state.system
    
    if system.failure_predictor and system.failure_predictor.is_trained:
        # Use trained model
        results = system.failure_predictor.predict_with_learning(df)
    else:
        # Simulate failure prediction
        results = simulate_failure_prediction(df, threshold)
    
    return results

def run_zeroday_detection(df, sensitivity):
    """Run zero-day detection on the dataset"""
    
    system = st.session_state.system
    
    if system.zero_day_detector and hasattr(system.zero_day_detector, 'is_trained'):
        if system.zero_day_detector.is_trained:
            # Use trained model
            results = system.zero_day_detector.detect_threats(df)
        else:
            # Simulate zero-day detection
            results = simulate_zeroday_detection(df, sensitivity)
    else:
        # Simulate zero-day detection
        results = simulate_zeroday_detection(df, sensitivity)
    
    return results

def simulate_anomaly_detection(df, sensitivity):
    """Simulate anomaly detection results"""
    
    n_samples = len(df)
    
    # Create anomaly scores based on data patterns
    scores = []
    is_anomaly = []
    
    for _, row in df.iterrows():
        # Calculate anomaly score based on multiple factors
        cpu_score = max(0, (row['cpu_util'] - 80) / 20) if row['cpu_util'] > 80 else 0
        mem_score = max(0, (row['memory_util'] - 85) / 15) if row['memory_util'] > 85 else 0
        err_score = min(1, row['error_rate'] * 10) if row['error_rate'] > 0.05 else 0
        
        # Combined anomaly score
        anomaly_score = max(cpu_score, mem_score, err_score) + np.random.normal(0, 0.1)
        anomaly_score = np.clip(anomaly_score, 0, 1)
        
        # Adjust based on sensitivity
        adjusted_score = anomaly_score * (0.5 + sensitivity)
        adjusted_score = np.clip(adjusted_score, 0, 1)
        
        scores.append(adjusted_score)
        is_anomaly.append(1 if adjusted_score > 0.5 else 0)
    
    anomaly_count = sum(is_anomaly)
    
    return {
        'stacked_results': {
            'anomaly_scores': scores,
            'is_anomaly': is_anomaly,
            'anomaly_count': anomaly_count,
            'detection_rate': anomaly_count / n_samples * 100
        }
    }

def simulate_failure_prediction(df, threshold):
    """Simulate failure prediction results"""
    
    n_samples = len(df)
    
    # Predict failures based on resource utilization
    failure_probs = []
    failure_preds = []
    
    for _, row in df.iterrows():
        # Calculate failure probability
        cpu_factor = row['cpu_util'] / 100
        mem_factor = row['memory_util'] / 100
        err_factor = min(1, row['error_rate'] * 20)
        
        # Combined failure probability
        failure_prob = (cpu_factor * 0.4 + mem_factor * 0.4 + err_factor * 0.2) + np.random.normal(0, 0.1)
        failure_prob = np.clip(failure_prob, 0, 1)
        
        failure_probs.append(failure_prob)
        failure_preds.append(failure_prob > threshold)
    
    failure_count = sum(failure_preds)
    
    return {
        'failure_probabilities': failure_probs,
        'failure_predictions': failure_preds,
        'failure_count': failure_count,
        'failure_rate': failure_count / n_samples * 100,
        'threshold_used': threshold,
        'samples_processed': n_samples
    }

def simulate_zeroday_detection(df, sensitivity):
    """Simulate zero-day detection results"""
    
    n_samples = len(df)
    
    # Detect potential zero-day threats
    threat_scores = []
    is_threat = []
    
    for _, row in df.iterrows():
        # Calculate threat score (simplified)
        unusual_patterns = 0
        
        # Check for unusual CPU patterns
        if row['cpu_util'] > 95 or (row['cpu_util'] < 10 and row.get('network_in', 0) > 500):
            unusual_patterns += 1
        
        # Check for unusual error patterns
        if row['error_rate'] > 0.1:
            unusual_patterns += 1
        
        # Check for unusual network patterns (if available)
        if 'network_in' in row and 'network_out' in row:
            if row['network_in'] > 800 or row['network_out'] > 600:
                unusual_patterns += 1
        
        # Base threat score
        threat_score = (unusual_patterns / 3) * sensitivity + np.random.normal(0, 0.1)
        threat_score = np.clip(threat_score, 0, 1)
        
        threat_scores.append(threat_score)
        is_threat.append(1 if threat_score > 0.4 else 0)
    
    threat_count = sum(is_threat)
    
    return {
        'combined_threats': {
            'combined_scores': threat_scores,
            'is_threat': is_threat,
            'threat_count': threat_count,
            'detection_rate': threat_count / n_samples * 100
        }
    }

def display_analysis_results(df, results):
    """Display comprehensive analysis results"""
    
    st.markdown("---")
    st.markdown("## üìä Analysis Results")
    
    # Summary metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_samples = len(df)
        st.metric("Total Samples", f"{total_samples:,}")
    
    with col2:
        anomaly_count = 0
        if results.get('anomaly') and 'stacked_results' in results['anomaly']:
            anomaly_count = results['anomaly']['stacked_results'].get('anomaly_count', 0)
        st.metric("Anomalies Detected", anomaly_count, 
                 delta=f"{anomaly_count/total_samples*100:.1f}%")
    
    with col3:
        failure_count = 0
        if results.get('failure'):
            failure_count = results['failure'].get('failure_count', 0)
        st.metric("Failures Predicted", failure_count,
                 delta=f"{failure_count/total_samples*100:.1f}%")
    
    with col4:
        threat_count = 0
        if results.get('zeroday') and 'combined_threats' in results['zeroday']:
            threat_count = results['zeroday']['combined_threats'].get('threat_count', 0)
        st.metric("Zero-Day Threats", threat_count,
                 delta=f"{threat_count/total_samples*100:.1f}%")
    
    # Detailed results tabs
    tab1, tab2, tab3, tab4 = st.tabs(["üîç Anomalies", "‚ö†Ô∏è Failures", "üõ°Ô∏è Zero-Day", "üìà Summary"])
    
    with tab1:
        display_anomaly_results(df, results.get('anomaly'))
    
    with tab2:
        display_failure_results(df, results.get('failure'))
    
    with tab3:
        display_zeroday_results(df, results.get('zeroday'))
    
    with tab4:
        display_summary_results(df, results)

def display_anomaly_results(df, anomaly_results):
    """Display anomaly detection results"""
    
    if not anomaly_results or 'stacked_results' not in anomaly_results:
        st.warning("‚ö†Ô∏è No anomaly detection results available")
        return
    
    stacked = anomaly_results['stacked_results']
    scores = stacked.get('anomaly_scores', [])
    is_anomaly = stacked.get('is_anomaly', [])
    anomaly_count = stacked.get('anomaly_count', 0)
    
    st.markdown(f"### üìä Anomaly Detection Results")
    st.markdown(f"**Detected {anomaly_count} anomalies out of {len(df)} samples**")
    
    if scores and is_anomaly:
        # Create visualization
        viz_data = pd.DataFrame({
            'Sample': range(len(scores)),
            'Anomaly_Score': scores,
            'Is_Anomaly': is_anomaly
        })
        
        # Plot anomaly scores over time
        fig = px.line(viz_data, x='Sample', y='Anomaly_Score', 
                     title='Anomaly Scores Across Dataset')
        
        # Highlight detected anomalies
        anomaly_points = viz_data[viz_data['Is_Anomaly'] == 1]
        if not anomaly_points.empty:
            fig.add_scatter(x=anomaly_points['Sample'], 
                          y=anomaly_points['Anomaly_Score'],
                          mode='markers', marker=dict(color='red', size=8),
                          name='Detected Anomalies')
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Top anomalies table
        if anomaly_count > 0:
            st.markdown("#### üîù Top Anomalies")
            top_anomalies = []
            
            for i, (score, is_anom) in enumerate(zip(scores, is_anomaly)):
                if is_anom:
                    sample_data = df.iloc[i] if i < len(df) else {}
                    top_anomalies.append({
                        'Sample': i,
                        'Score': score,
                        'CPU_Util': sample_data.get('cpu_util', 'N/A'),
                        'Memory_Util': sample_data.get('memory_util', 'N/A'),
                        'Error_Rate': sample_data.get('error_rate', 'N/A')
                    })
            
            # Sort by score and show top 10
            top_anomalies.sort(key=lambda x: x['Score'], reverse=True)
            anomaly_df = pd.DataFrame(top_anomalies[:10])
            st.dataframe(anomaly_df, use_container_width=True)

def display_failure_results(df, failure_results):
    """Display failure prediction results"""
    
    if not failure_results:
        st.warning("‚ö†Ô∏è No failure prediction results available")
        return
    
    failure_probs = failure_results.get('failure_probabilities', [])
    failure_preds = failure_results.get('failure_predictions', [])
    failure_count = failure_results.get('failure_count', 0)
    threshold = failure_results.get('threshold_used', 0.5)
    
    st.markdown(f"### ‚ö†Ô∏è Failure Prediction Results")
    st.markdown(f"**Predicted {failure_count} potential failures (threshold: {threshold:.2f})**")
    
    if failure_probs:
        # Create visualization
        viz_data = pd.DataFrame({
            'Sample': range(len(failure_probs)),
            'Failure_Probability': failure_probs,
            'Will_Fail': failure_preds
        })
        
        # Plot failure probabilities
        fig = px.line(viz_data, x='Sample', y='Failure_Probability',
                     title='Failure Probabilities Across Dataset')
        
        # Add threshold line
        fig.add_hline(y=threshold, line_dash="dash", line_color="red",
                     annotation_text=f"Threshold ({threshold:.2f})")
        
        # Highlight predicted failures
        failure_points = viz_data[viz_data['Will_Fail'] == True]
        if not failure_points.empty:
            fig.add_scatter(x=failure_points['Sample'],
                          y=failure_points['Failure_Probability'],
                          mode='markers', marker=dict(color='red', size=8),
                          name='Predicted Failures')
        
        st.plotly_chart(fig, use_container_width=True)

def display_zeroday_results(df, zeroday_results):
    """Display zero-day detection results"""
    
    if not zeroday_results or 'combined_threats' not in zeroday_results:
        st.warning("‚ö†Ô∏è No zero-day detection results available")
        return
    
    combined = zeroday_results['combined_threats']
    threat_scores = combined.get('combined_scores', [])
    is_threat = combined.get('is_threat', [])
    threat_count = combined.get('threat_count', 0)
    
    st.markdown(f"### üõ°Ô∏è Zero-Day Detection Results") 
    st.markdown(f"**Detected {threat_count} potential zero-day threats**")
    
    if threat_scores:
        # Create visualization
        viz_data = pd.DataFrame({
            'Sample': range(len(threat_scores)),
            'Threat_Score': threat_scores,
            'Is_Threat': is_threat
        })
        
        # Plot threat scores
        fig = px.line(viz_data, x='Sample', y='Threat_Score',
                     title='Zero-Day Threat Scores Across Dataset')
        
        # Highlight detected threats
        threat_points = viz_data[viz_data['Is_Threat'] == 1]
        if not threat_points.empty:
            fig.add_scatter(x=threat_points['Sample'],
                          y=threat_points['Threat_Score'],
                          mode='markers', marker=dict(color='red', size=8),
                          name='Detected Threats')
        
        st.plotly_chart(fig, use_container_width=True)

def display_summary_results(df, results):
    """Display summary of all results"""
    
    st.markdown("### üìà Analysis Summary")
    
    # Create summary dataframe
    summary_data = []
    
    if results.get('anomaly'):
        anom = results['anomaly'].get('stacked_results', {})
        summary_data.append({
            'Analysis Type': 'üîç Anomaly Detection',
            'Detections': anom.get('anomaly_count', 0),
            'Detection Rate': f"{anom.get('detection_rate', 0):.1f}%",
            'Status': '‚úÖ Completed' if anom else '‚ùå Failed'
        })
    
    if results.get('failure'):
        fail = results['failure']
        summary_data.append({
            'Analysis Type': '‚ö†Ô∏è Failure Prediction',
            'Detections': fail.get('failure_count', 0),
            'Detection Rate': f"{fail.get('failure_rate', 0):.1f}%",
            'Status': '‚úÖ Completed' if fail else '‚ùå Failed'
        })
    
    if results.get('zeroday'):
        zero = results['zeroday'].get('combined_threats', {})
        summary_data.append({
            'Analysis Type': 'üõ°Ô∏è Zero-Day Detection', 
            'Detections': zero.get('threat_count', 0),
            'Detection Rate': f"{zero.get('detection_rate', 0):.1f}%",
            'Status': '‚úÖ Completed' if zero else '‚ùå Failed'
        })
    
    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        st.dataframe(summary_df, use_container_width=True, hide_index=True)
    
    # Export options
    st.markdown("### üì§ Export Results")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üíæ Download CSV Report"):
            # Create comprehensive report
            report_data = df.copy()
            
            if results.get('anomaly') and 'stacked_results' in results['anomaly']:
                stacked = results['anomaly']['stacked_results']
                report_data['anomaly_score'] = stacked.get('anomaly_scores', [0] * len(df))
                report_data['is_anomaly'] = stacked.get('is_anomaly', [0] * len(df))
            
            if results.get('failure'):
                fail = results['failure']
                report_data['failure_probability'] = fail.get('failure_probabilities', [0] * len(df))
                report_data['will_fail'] = fail.get('failure_predictions', [False] * len(df))
            
            if results.get('zeroday') and 'combined_threats' in results['zeroday']:
                zero = results['zeroday']['combined_threats']
                report_data['threat_score'] = zero.get('combined_scores', [0] * len(df))
                report_data['is_threat'] = zero.get('is_threat', [0] * len(df))
            
            csv = report_data.to_csv(index=False)
            st.download_button(
                label="üì• Download Report",
                data=csv,
                file_name=f"sre_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
    
    with col2:
        if st.button("üìä Download JSON Report"):
            json_report = {
                'timestamp': datetime.now().isoformat(),
                'dataset_info': {
                    'total_samples': len(df),
                    'columns': list(df.columns),
                    'data_types': df.dtypes.astype(str).to_dict()
                },
                'analysis_results': results,
                'summary': summary_data
            }
            
            json_str = json.dumps(json_report, indent=2, default=str)
            st.download_button(
                label="üì• Download JSON",
                data=json_str,
                file_name=f"sre_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
    
    with col3:
        st.info("üìß Email reports and scheduled analysis coming soon!")

def train_models_on_data(df):
    """Train models on the uploaded dataset"""
    
    with st.spinner("üß† Training models on your dataset..."):
        try:
            system = st.session_state.system
            
            # Prepare data for training
            training_data = prepare_data_for_analysis(df.copy())
            
            # Add timestamp if not present
            if 'timestamp' not in training_data.columns:
                training_data['timestamp'] = pd.date_range(
                    start=datetime.now(), periods=len(training_data), freq='1min'
                )
            
            # Create enhanced context (simplified version)
            training_data['log_error_rate'] = training_data['error_rate']
            training_data['chat_activity'] = 0.0
            training_data['ticket_pressure'] = 0.0
            training_data['system_stability_score'] = 100 - ((training_data['cpu_util'] + training_data['memory_util']) / 2)
            training_data['incident_risk_score'] = training_data['error_rate']
            
            # Progress tracking
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Step 1: Train self-learning system
            status_text.text("üöÄ Training self-learning system...")
            progress_bar.progress(50)
            
            training_results = system.train_self_learning_system(training_data)
            
            progress_bar.progress(100)
            status_text.text("‚úÖ Training completed!")
            
            time.sleep(1)
            progress_bar.empty()
            status_text.empty()
            
            if training_results.get('overall_status') == 'success':
                st.success("üéâ Model training completed successfully!")
                
                # Show training results
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if training_results.get('anomaly'):
                        st.metric("üîç Anomaly Model", "‚úÖ Trained")
                    else:
                        st.metric("üîç Anomaly Model", "‚ö†Ô∏è Partial")
                
                with col2:
                    if training_results.get('failure'):
                        acc = training_results['failure'].get('accuracy', 0) * 100
                        st.metric("‚ö†Ô∏è Failure Model", f"‚úÖ {acc:.1f}% Acc")
                    else:
                        st.metric("‚ö†Ô∏è Failure Model", "‚ö†Ô∏è Partial")
                
                with col3:
                    if training_results.get('zero_day'):
                        st.metric("üõ°Ô∏è Zero-Day Model", "‚úÖ Trained")
                    else:
                        st.metric("üõ°Ô∏è Zero-Day Model", "‚ö†Ô∏è Partial")
                
                # Save models
                status_text.text("üíæ Saving trained models...")
                system.save_self_learning_models()
                st.session_state.models_loaded = True
                
                # Show knowledge base growth
                if system.knowledge_base:
                    kb = system.knowledge_base
                    st.markdown("### üß† Knowledge Base Growth")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("Anomaly Patterns", len(kb.known_anomaly_patterns))
                    with col2:
                        st.metric("Failure Patterns", len(kb.failure_patterns))
                    with col3:
                        st.metric("Zero-Day Patterns", len(kb.zero_day_patterns))
                
                st.info("üí° Models are now trained on your data and ready for improved predictions!")
            else:
                st.warning("‚ö†Ô∏è Training completed with some issues. Check the logs for details.")
            
        except Exception as e:
            st.error(f"‚ùå Training failed: {e}")

def extract_patterns_from_data(df):
    """Extract patterns from the dataset without full training"""
    
    with st.spinner("üîç Extracting patterns from your dataset..."):
        try:
            # Analyze data patterns
            patterns = analyze_data_patterns(df)
            
            st.success("‚úÖ Pattern extraction completed!")
            
            # Display discovered patterns
            st.markdown("### üîç Discovered Patterns")
            
            tab1, tab2, tab3 = st.tabs(["üìä Statistical Patterns", "‚ö†Ô∏è Anomaly Patterns", "üîÑ Correlation Patterns"])
            
            with tab1:
                display_statistical_patterns(df, patterns)
            
            with tab2:
                display_anomaly_patterns(df, patterns)
            
            with tab3:
                display_correlation_patterns(df, patterns)
            
        except Exception as e:
            st.error(f"‚ùå Pattern extraction failed: {e}")

def analyze_data_patterns(df):
    """Analyze patterns in the dataset"""
    
    patterns = {
        'statistical': {},
        'anomaly': {},
        'correlation': {}
    }
    
    # Statistical patterns
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        patterns['statistical'][col] = {
            'mean': df[col].mean(),
            'std': df[col].std(),
            'min': df[col].min(),
            'max': df[col].max(),
            'q95': df[col].quantile(0.95),
            'outlier_threshold': df[col].quantile(0.95) + 1.5 * df[col].std()
        }
    
    # Anomaly patterns - identify unusual combinations
    if 'cpu_util' in df.columns and 'memory_util' in df.columns:
        high_resource_usage = df[
            (df['cpu_util'] > df['cpu_util'].quantile(0.9)) & 
            (df['memory_util'] > df['memory_util'].quantile(0.9))
        ]
        patterns['anomaly']['high_resource_usage'] = len(high_resource_usage)
    
    # Correlation patterns
    if len(numeric_cols) > 1:
        correlation_matrix = df[numeric_cols].corr()
        patterns['correlation']['matrix'] = correlation_matrix
        
        # Find strong correlations
        strong_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:
                    strong_correlations.append({
                        'feature1': correlation_matrix.columns[i],
                        'feature2': correlation_matrix.columns[j],
                        'correlation': corr_value
                    })
        
        patterns['correlation']['strong_correlations'] = strong_correlations
    
    return patterns

def display_statistical_patterns(df, patterns):
    """Display statistical patterns"""
    
    st.markdown("#### üìà Statistical Summary")
    
    if patterns.get('statistical'):
        stats_data = []
        for col, stats in patterns['statistical'].items():
            stats_data.append({
                'Feature': col,
                'Mean': f"{stats['mean']:.2f}",
                'Std Dev': f"{stats['std']:.2f}",
                'Min': f"{stats['min']:.2f}",
                'Max': f"{stats['max']:.2f}",
                '95th Percentile': f"{stats['q95']:.2f}"
            })
        
        stats_df = pd.DataFrame(stats_data)
        st.dataframe(stats_df, use_container_width=True, hide_index=True)

def display_anomaly_patterns(df, patterns):
    """Display anomaly patterns"""
    
    st.markdown("#### üö® Anomaly Patterns")
    
    if patterns.get('anomaly'):
        for pattern_name, count in patterns['anomaly'].items():
            st.metric(pattern_name.replace('_', ' ').title(), count)
    
    # Show distribution plots for key metrics
    numeric_cols = df.select_dtypes(include=[np.number]).columns[:3]  # Show first 3
    
    for col in numeric_cols:
        fig = px.histogram(df, x=col, title=f'Distribution of {col}')
        
        # Add threshold lines if available
        if col in patterns.get('statistical', {}):
            stats = patterns['statistical'][col]
            fig.add_vline(x=stats['q95'], line_dash="dash", line_color="red",
                         annotation_text="95th percentile")
        
        st.plotly_chart(fig, use_container_width=True)

def display_correlation_patterns(df, patterns):
    """Display correlation patterns"""
    
    st.markdown("#### üîó Feature Correlations")
    
    if 'correlation' in patterns and 'matrix' in patterns['correlation']:
        corr_matrix = patterns['correlation']['matrix']
        
        # Display correlation heatmap
        fig = px.imshow(corr_matrix, 
                       title="Feature Correlation Matrix",
                       color_continuous_scale="RdBu_r",
                       aspect="auto")
        st.plotly_chart(fig, use_container_width=True)
        
        # Show strong correlations
        if 'strong_correlations' in patterns['correlation']:
            strong_corrs = patterns['correlation']['strong_correlations']
            
            if strong_corrs:
                st.markdown("#### üí™ Strong Correlations (|r| > 0.7)")
                corr_data = []
                for corr in strong_corrs:
                    corr_data.append({
                        'Feature 1': corr['feature1'],
                        'Feature 2': corr['feature2'],
                        'Correlation': f"{corr['correlation']:.3f}",
                        'Strength': 'Strong Positive' if corr['correlation'] > 0.7 else 'Strong Negative'
                    })
                
                corr_df = pd.DataFrame(corr_data)
                st.dataframe(corr_df, use_container_width=True, hide_index=True)
            else:
                st.info("No strong correlations found (|r| > 0.7)")
    else:
        st.info("Correlation analysis requires multiple numeric features")